{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12972891,"sourceType":"datasetVersion","datasetId":8210902}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers\n!pip install datasets\n!pip install evaluate\n!pip install git+https://github.com/csebuetnlp/normalizer\n!pip install -q transformers datasets accelerate\n","metadata":{"_uuid":"07a70ba9-a45d-4634-a58b-821cebb3e3b9","_cell_guid":"34e53954-2a8d-44c1-bd50-e59adced7588","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import logging\nimport os\nimport sys\nfrom typing import Dict, List\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom datasets import Dataset, DatasetDict\nfrom transformers import (\n    AutoConfig,\n    AutoModelForSequenceClassification,\n    AutoTokenizer,\n    Trainer,\n    TrainingArguments,\n    default_data_collator,\n)\nfrom normalizer import normalize\nfrom sklearn.metrics import f1_score, confusion_matrix\n\n# Disable W&B\n# os.environ[\"WANDB_DISABLED\"] = \"true\"\nos.environ[\"WANDB_MODE\"] = \"disabled\"\n\n\nlogging.basicConfig(\n    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n    datefmt=\"%m/%d/%Y %H:%M:%S\",\n    handlers=[logging.StreamHandler(sys.stdout)],\n    level=logging.INFO,\n)\nlogger = logging.getLogger(__name__)\n\n\n# File paths (same style as finetune_subtask_1B.py; expect files next to script or CWD)\nTRAIN_1B = '/kaggle/input/subtask1-a/blp25_hatespeech_subtask_1A_train.tsv'\nDEV_1B = '/kaggle/input/subtask1-a/blp25_hatespeech_subtask_1A_dev.tsv'\nTEST_1B = '/kaggle/input/subtask1-a/blp25_hatespeech_subtask_1A_dev_test.tsv'\n\n\n# Label mappings\nL2ID_1B = {'None': 0, 'Religious Hate': 1, 'Sexism': 2, 'Political Hate': 3, 'Profane': 4, 'Abusive': 5}\n\nMODEL_NAME_STAGE1 = \"csebuetnlp/banglabert\"\nMODEL_NAME_STAGE2 = \"csebuetnlp/banglabert\"\n\nMAX_SEQ_LEN = 256\n\n\ndef load_1b_binary_datasets() -> DatasetDict:\n    \"\"\"Load 1B data and convert to binary labels: 0 -> None, 1 -> Hate (any non-None).\"\"\"\n    train_df = pd.read_csv(TRAIN_1B, sep=\"\\t\")\n    dev_df = pd.read_csv(DEV_1B, sep=\"\\t\")\n    test_df = pd.read_csv(TEST_1B, sep=\"\\t\")\n\n    def to_binary(df: pd.DataFrame) -> pd.DataFrame:\n        mapped = df.copy()\n        mapped[\"label\"] = df[\"label\"].map(L2ID_1B).fillna(0).astype(int).apply(lambda x: 0 if x == 0 else 1)\n        return mapped\n\n    train_df = to_binary(train_df)\n    dev_df = to_binary(dev_df)\n    # test has no labels; keep as-is\n\n    return DatasetDict(\n        {\n            \"train\": Dataset.from_pandas(train_df),\n            \"validation\": Dataset.from_pandas(dev_df),\n            \"test\": Dataset.from_pandas(test_df),\n        }\n    )\n\n\ndef load_1b_multiclass_datasets() -> DatasetDict:\n    \"\"\"Load 1B data; keep original 5 labels with 0 as None.\"\"\"\n    train_df = pd.read_csv(TRAIN_1B, sep=\"\\t\")\n    dev_df = pd.read_csv(DEV_1B, sep=\"\\t\")\n    test_df = pd.read_csv(TEST_1B, sep=\"\\t\")\n\n    for df in (train_df, dev_df):\n        df[\"label\"] = df[\"label\"].map(L2ID_1B).fillna(0).astype(int)\n\n    return DatasetDict(\n        {\n            \"train\": Dataset.from_pandas(train_df),\n            \"validation\": Dataset.from_pandas(dev_df),\n            \"test\": Dataset.from_pandas(test_df),\n        }\n    )\n\n\ndef build_tokenizer_and_model(model_name: str, num_labels: int):\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    config = AutoConfig.from_pretrained(model_name, num_labels=num_labels)\n    model = AutoModelForSequenceClassification.from_pretrained(model_name, config=config)\n    return tokenizer, model\n\n\ndef preprocess_dataset(ds: DatasetDict, tokenizer, max_len: int) -> DatasetDict:\n    def preprocess(examples):\n        texts = [normalize(str(t)) for t in examples[\"text\"]]\n        result = tokenizer(texts, padding=\"max_length\", truncation=True, max_length=max_len)\n        if \"label\" in examples:\n            result[\"label\"] = examples[\"label\"]\n        return result\n\n    processed = DatasetDict()\n    for split, split_ds in ds.items():\n        # keep id for later routing; we will drop on Trainer input when needed\n        processed[split] = split_ds.map(preprocess, batched=True, desc=f\"Tokenizing {split}\")\n    return processed\n\n\ndef train_stage(dataset: DatasetDict, tokenizer, model, output_dir: str) -> Trainer:\n    train_dataset = dataset[\"train\"]\n    eval_dataset = dataset[\"validation\"]\n\n    # Drop id for Trainer\n    if \"id\" in train_dataset.column_names:\n        train_dataset = train_dataset.remove_columns(\"id\")\n    if \"id\" in eval_dataset.column_names:\n        eval_dataset = eval_dataset.remove_columns(\"id\")\n\n    # args = TrainingArguments(\n    #     output_dir=output_dir,\n    #     overwrite_output_dir=True,\n    #     save_strategy=\"epoch\",\n    #     eval_strategy=\"epoch\",\n    #     save_total_limit=2,\n    #     per_device_train_batch_size=16,\n    #     per_device_eval_batch_size=16,\n    #     gradient_accumulation_steps=2,\n    #     learning_rate=3e-5,\n    #     num_train_epochs=3,\n    #     weight_decay=0.01,\n    #     logging_dir=os.path.join(output_dir, \"logs\"),\n    #     logging_steps=50,\n    #     report_to=None,\n    #     load_best_model_at_end=True,\n    #     metric_for_best_model=\"eval_f1\",\n    #     greater_is_better=True,\n    #     fp16=torch.cuda.is_available(),\n    #     warmup_ratio=0.08,\n    # )\n\n    args = TrainingArguments(\n        output_dir=output_dir,\n        overwrite_output_dir=False,\n        save_strategy=\"steps\",\n        save_steps=200,\n        save_total_limit=2,\n    \n        eval_strategy=\"steps\",\n        eval_steps=200,\n        logging_dir=output_dir,\n        logging_strategy=\"steps\",\n        logging_steps=200,\n        report_to=None,\n    \n        load_best_model_at_end=True,\n        metric_for_best_model=\"eval_f1\",\n    \n        fp16=True,\n        learning_rate=3e-5,\n        warmup_ratio=0.1,\n        weight_decay=0.01,\n        label_smoothing_factor=0.1,\n        lr_scheduler_type=\"linear\",\n    \n        gradient_accumulation_steps=4,\n        per_device_train_batch_size=4,\n        per_device_eval_batch_size=4,\n    \n        num_train_epochs=2\n    )\n\n\n    def compute_metrics(p):\n        preds = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions\n        preds = np.argmax(preds, axis=1)\n        acc = (preds == p.label_ids).astype(np.float32).mean().item()\n        cm = confusion_matrix(p.label_ids, preds)\n        print(\"Confusion matrix:\\n\", cm)\n        return {\"accuracy\": acc, \"f1\": f1_score(p.label_ids, preds, average=\"micro\")}\n\n    trainer = Trainer(\n        model=model,\n        args=args,\n        train_dataset=train_dataset,\n        eval_dataset=eval_dataset,\n        tokenizer=tokenizer,\n        data_collator=default_data_collator,\n        compute_metrics=compute_metrics,\n    )\n\n    trainer.train()\n    trainer.save_model()\n    return trainer\n\n\ndef route_indices_by_stage1(pred_probs: np.ndarray, ids: List, threshold: float = 0.5):\n    # pred_probs is probability for class 1 (hate)\n    is_hate = pred_probs >= threshold\n    hate_idx = np.where(is_hate)[0]\n    none_idx = np.where(~is_hate)[0]\n    return hate_idx, none_idx\n\n\ndef run_cascade(threshold: float = 0.5):\n    # Stage 1: 1B -> binary (None vs Hate)\n    logger.info(\"Loading 1B (binary) datasets...\")\n    ds1 = load_1b_binary_datasets()\n    tok1, model1 = build_tokenizer_and_model(MODEL_NAME_STAGE1, num_labels=2)\n    ds1 = preprocess_dataset(ds1, tok1, MAX_SEQ_LEN)\n    trainer1 = train_stage(ds1, tok1, model1, output_dir=\"/kaggle/working/cascade_stage1_1B_binary\")\n\n    # Stage 2: 1B -> 4 hate classes (trained on hate-only)\n    logger.info(\"Loading 1B (multiclass) datasets...\")\n    ds2 = load_1b_multiclass_datasets()\n    # Stage 2 will be trained on hate-only with labels remapped to 0..3\n    tok2, model2 = build_tokenizer_and_model(MODEL_NAME_STAGE2, num_labels=4)\n    ds2 = preprocess_dataset(ds2, tok2, MAX_SEQ_LEN)\n\n    # Filter Stage 2 training to hate-only labels (exclude None=0)\n    train2 = ds2[\"train\"]\n    eval2 = ds2[\"validation\"]\n    test2 = ds2[\"test\"]\n\n    train2_hate = train2.filter(lambda eg: eg[\"label\"] != 0).map(lambda eg: {\"label\": eg[\"label\"] - 1})\n    eval2_hate = eval2.filter(lambda eg: eg.get(\"label\", 0) != 0).map(lambda eg: {\"label\": eg[\"label\"] - 1})\n\n    # Train stage 2 on hate-only\n    stage2_train_ds = DatasetDict({\"train\": train2_hate, \"validation\": eval2_hate, \"test\": test2})\n    trainer2 = train_stage(stage2_train_ds, tok2, model2, output_dir=\"/kaggle/working/cascade_stage2_1B_hate\")\n\n    # Cascaded inference on 1B dev and test\n    id2l_1B = {v: k for k, v in L2ID_1B.items()}\n\n    def predict_stage1(texts: List[str]) -> np.ndarray:\n        enc = tok1([normalize(str(t)) for t in texts], padding=True, truncation=True, max_length=MAX_SEQ_LEN, return_tensors=\"pt\")\n        with torch.no_grad():\n            outputs = trainer1.model(**{k: v.to(trainer1.model.device) for k, v in enc.items()})\n            logits = outputs.logits.detach().cpu().numpy()\n            probs = torch.softmax(torch.tensor(logits), dim=-1).numpy()[:, 1]\n        return probs\n\n    def predict_stage2(texts: List[str]) -> np.ndarray:\n        enc = tok2([normalize(str(t)) for t in texts], padding=True, truncation=True, max_length=MAX_SEQ_LEN, return_tensors=\"pt\")\n        with torch.no_grad():\n            outputs = trainer2.model(**{k: v.to(trainer2.model.device) for k, v in enc.items()})\n            logits = outputs.logits.detach().cpu().numpy()\n            preds = np.argmax(logits, axis=1)\n        return preds\n\n    # Dev routing\n    dev_df_raw = pd.read_csv(DEV_1B, sep=\"\\t\")\n    dev_ids = dev_df_raw[\"id\"].tolist()\n    dev_texts = dev_df_raw[\"text\"].astype(str).tolist()\n    dev_probs = predict_stage1(dev_texts)\n    hate_idx, none_idx = route_indices_by_stage1(dev_probs, dev_ids, threshold)\n\n    dev_final = np.zeros(len(dev_texts), dtype=int)\n    dev_final[none_idx] = 0\n    if len(hate_idx) > 0:\n        sub_preds = predict_stage2([dev_texts[i] for i in hate_idx])  # 0..3\n        dev_final[hate_idx] = sub_preds + 1  # map back to 1..4\n\n    # Save dev predictions\n    dev_out = os.path.join(\"/kaggle/working/cascade_outputs\", \"subtask_1B_dev_cascade.tsv\")\n    os.makedirs(os.path.dirname(dev_out), exist_ok=True)\n    with open(dev_out, \"w\", encoding=\"utf-8\") as w:\n        w.write(\"id\\tlabel\\tmodel\\n\")\n        for i, pid in enumerate(dev_ids):\n            w.write(f\"{pid}\\t{id2l_1B[int(dev_final[i])]}\\tcascade(banglabert->banglabert)\\n\")\n    logger.info(f\"Saved cascaded dev predictions to {dev_out}\")\n\n    # Test routing\n    test_df_raw = pd.read_csv(TEST_1B, sep=\"\\t\")\n    test_ids = test_df_raw[\"id\"].tolist()\n    test_texts = test_df_raw[\"text\"].astype(str).tolist()\n    test_probs = predict_stage1(test_texts)\n    hate_idx_t, none_idx_t = route_indices_by_stage1(test_probs, test_ids, threshold)\n\n    test_final = np.zeros(len(test_texts), dtype=int)\n    test_final[none_idx_t] = 0\n    if len(hate_idx_t) > 0:\n        sub_preds_t = predict_stage2([test_texts[i] for i in hate_idx_t])  # 0 to 3\n        test_final[hate_idx_t] = sub_preds_t + 1  # map back to 1..4\n\n    test_out = os.path.join(\"/kaggle/working/\", \"subtask_1B_test_cascade.tsv\")\n    with open(test_out, \"w\", encoding=\"utf-8\") as w:\n        w.write(\"id\\tlabel\\tmodel\\n\")\n        for i, pid in enumerate(test_ids):\n            w.write(f\"{pid}\\t{id2l_1B[int(test_final[i])]}\\tcascade(banglabert->banglabert)\\n\")\n    logger.info(f\"Saved cascaded test predictions to {test_out}\")\n\n\nif __name__ == \"__main__\":\n    run_cascade(threshold=0.5)","metadata":{"_uuid":"7e3316db-1b21-4056-b32d-2bdb79a9321b","_cell_guid":"2faa0f66-df83-4692-894c-0b8c813ea9d5","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null}]}